# ðŸ§  First Principles Deep Learning

This project explores the **core mathematical building blocks of neural networks** â€” starting entirely from scratch using NumPy, and explained from first principles.

Itâ€™s designed to **demystify neural nets** by showing how every part works: from the dot product to gradient descent, all coded by hand.

---

## ðŸ“š Topics Covered

| Concept               | Description                                 |
| --------------------- | ------------------------------------------- |
| âœ… Dot Product        | Foundation of every neuron's calculation    |
| âœ… Sigmoid & Tanh     | Activation functions + their derivatives    |
| âœ… Softmax            | Probability distribution for classification |
| âœ… Cross-Entropy Loss | How classification models learn             |
| âœ… MSE                | Error measurement for regression tasks      |
| âœ… Gradient Descent   | Core optimization method                    |
| âœ… Chain Rule         | How backpropagation works mathematically    |

---

## ðŸ›  Built With

- Python 3
- NumPy
- Matplotlib (for optional visualizations)

---

## ðŸ“‚ Structure

```bash
first-principles-deep-learning/
â”œâ”€â”€ dot_product.py
â”œâ”€â”€ sigmoid_and_derivative.py
â”œâ”€â”€ tanh_and_derivative.py
â”œâ”€â”€ softmax.py
â”œâ”€â”€ cross_entropy.py
â”œâ”€â”€ mse.py
â”œâ”€â”€ gradient_descent.py
â”œâ”€â”€ chain_rule.py
â”œâ”€â”€ simple_neural_net.py
â””â”€â”€ README.md


## ðŸ§  Neural Network Structure

This diagram shows the complete flow of a neural network from data loading to evaluation.

![Neural Network Structure](/images/nuralnet.png)
```
